---
layout: ../layouts/Layout.astro
title: Teaching LLMs How to Learn with Contextual Fine-Tuning
description: Simple project page template for your research paper, built with Astro and Tailwind CSS
favicon: favicon.svg
thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import mainfig from "../assets/mainfig.webp";
import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {},{},
    {
      name: "Younwoo Choi",
      notes: ["*"],
    },
    {
      name: "Muhammad Adil Asif",
      notes: ["*"],
    },
    {
      name: "Ziwen Han",
      notes: ["†"],
    },
    {
      name: "John Willes",
    },
    {},{},{},{},{},{},{},{},{},{},{},{},{},{},
    {
      name: "Rahul G. Krishnan",
    },
    {},{},{},{},{},{},{},{},{},
    {
      name: "University of Toronto"
    },
    {
      name: "Vector Institute"
    },
  ]}
  conference="ICLR 2025"
  notes={[
    {
      symbol: "*",
      text: "Equal contribution",
    },
    {
      symbol: "†",
      text: "Work conducted while at the Vector Institute",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "https://openreview.net/forum?id=FS2nukC2jv",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://github.com/younwoochoi/Contextual-Fine-Tuning",
      icon: "ri:github-line",
    },
    {
      name: "Dataset",
      url: "https://huggingface.co/datasets/ywchoi/OpenMedText",
      icon: "simple-icons:huggingface"
    }
  ]}
  />

<Image source={mainfig} altText="Diagram of the transformer deep learning architecture." />
{/*  ><Video source={outside} */}

<HighlightedSection>

## Abstract

Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human's learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, "can prompting help us teach LLMs how to learn". In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model’s interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains.

</HighlightedSection>

## Figures

Use the figure component to display images, videos, equations, or any other element, with an optional caption.

<Figure
    caption="Diagram of the transformer deep learning architecture."
  >
    <Image source={transformer} altText="Diagram of the transformer deep learning architecture." />
</Figure>

## Two columns

Use the two columns component to display two columns of content. In this example, the first column contains a figure with a YouTube video and the second column contains a figure with a custom [React](https://react.dev/) component. By default, they display side by side, but if the screen is narrow enough (for example, on mobile), they're arranged vertically.

<TwoColumns>
  <Figure slot="left" caption="Take a look at this YouTube video.">
    <YouTubeVideo videoId="wjZofJX0v4M" />
  </Figure>
  <Figure slot="right" caption="Now look at this Gaussian Splat, rendered with a React component.">
    <Splat client:idle />
  </Figure>
</TwoColumns>

## Heading levels

Use headings to divide your content into sections.

### Heading 3

Go down a level to heading 3...

#### Heading 4

...and down again to heading 4.

## LaTeX

You can also add LaTeX formulas, rendered during the build process using [KaTeX](https://katex.org/) so they're quick to load for visitors of your project page. You can write them inline, like this: <LaTeX inline formula="a^2 + b^2 = c^2" />. Or, you can write them as a block:

<LaTeX formula="\int_a^b f(x) dx" />

## Tables

You can add simple tables using [GitHub Flavored Markdown syntax](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/organizing-information-with-tables):

| Model | Accuracy | F1 score | Training time (hours) |
| :--- | :---: | :---: | :---: |
| BERT-base | 0.89 | 0.87 | 4.5 |
| RoBERTa-large | 0.92 | 0.91 | 7.2 |
| DistilBERT | 0.86 | 0.84 | 2.1 |
| XLNet | 0.90 | 0.89 | 6.8 |


## Method

We explore:
1. **Fixed, manually designed prompts** inspired by strategies for effective human learning (focusing on key concepts, critical analysis, bridging new and old knowledge, etc.).
2. **Text-adaptive prompts** generated automatically by another LLM (GPT-4-style model), which tailors the prompt based on the current training text.

## OpenMedText

To test domain adaptation in a challenging setting, we curated a biomedical dataset of 121,489 biomedical journal articles (MDPI) and 29 open-source medical textbooks. This differs from PubMed or PMC because it includes educational textbooks with explanatory text. We release <em>OpenMedText</em> as a resource for domain adaptation research.


## Experiments

We evaluate on both **financial** and **medical** tasks to show the effectiveness of CFT. 

**Financial tasks** include FiQA sentiment analysis, MultiFin headline classification, and Causal20 (classifying cause vs. noise).  
**Medical tasks** include subsets of MMLU (Anatomy, Clinical Knowledge, College Biology, College Medicine, Medical Genetics, Professional Medicine) and the MedQA professional exam dataset.

### Setup

- We fine-tune LLaMA2-based models of size 7B and 13B.  
- We compare baseline Chat (no further domain adaptation), Chat (CPT), and Chat (CFT) for each domain’s training corpus.  
- We measure zero-shot performance on the downstream tasks.

### Results

Below are sample results (see paper for full details).

<Figure caption="Medical Benchmarks (Zero-shot) using Llama 2 7B.">
  <Table>
    <thead>
      <tr>
        <th>Method</th>
        <th>Anatomy</th>
        <th>Clinical</th>
        <th>College Bio</th>
        <th>College Med</th>
        <th>Med Genetics</th>
        <th>Prof Med</th>
        <th>MedQA</th>
        <th>Avg</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Chat (baseline)</td>
        <td>44.07</td>
        <td>46.79</td>
        <td>48.61</td>
        <td>39.02</td>
        <td>49.00</td>
        <td>48.90</td>
        <td>38.96</td>
        <td>45.05</td>
      </tr>
      <tr>
        <td>Chat (CPT)</td>
        <td>45.19</td>
        <td>47.17</td>
        <td>49.31</td>
        <td>43.93</td>
        <td>50.50</td>
        <td>46.32</td>
        <td>39.28</td>
        <td>45.96</td>
      </tr>
      <tr>
        <td><strong>Chat (CFT)</strong></td>
        <td><strong>48.15</strong></td>
        <td><strong>48.87</strong></td>
        <td><strong>52.08</strong></td>
        <td><strong>44.22</strong></td>
        <td><strong>54.00</strong></td>
        <td>46.69</td>
        <td><strong>40.65</strong></td>
        <td><strong>47.81</strong></td>
      </tr>
    </tbody>
  </Table>
</Figure>

We see consistent gains from CFT. Similar trends appear at 13B scale, and for th


## BibTeX citation

```bibtex
@inproceedings{
choi2025teaching,
title={Teaching {LLM}s How To Learn with Contextual Fine-Tuning},
author={Younwoo Choi and Muhammad Adil Asif and Ziwen Han and John Willes and Rahul Krishnan},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=FS2nukC2jv}
}
```